{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP based Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from queue import  Queue\n",
    "from copy import  copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections\n",
      "   - appList\n",
      "   - appDetail\n",
      "   - userOwns\n",
      "   - userAchieve\n",
      "   - userSummary\n",
      "   - userBan\n",
      "   - appReview\n"
     ]
    }
   ],
   "source": [
    "cl = pymongo.MongoClient()\n",
    "db = cl.steam\n",
    "print(\"Collections\")\n",
    "for i in db.collection_names():\n",
    "    print(\"   - \"+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: is userOwns-data ordered with user-purchaed time ?\n",
    "## Q: 유저게임소유데이터( userOwns-data) 는 유저 구매순으로 정렬되어 있는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userOwnsMoreThan1games = db.userOwns.find({\"game_count\":{\"$gte\":1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uomt1Size = userOwnsMoreThan1games.count()\n",
    "x1_x2_y = []\n",
    "for _ in range(uomt1Size):\n",
    "    games = userOwnsMoreThan1games.next()['games']\n",
    "    x1 = -1\n",
    "    for x in map(lambda x:x[\"appid\"],games):\n",
    "        if x1 == -1:\n",
    "            x1 = x\n",
    "            continue\n",
    "        x1_x2_y.append([x1,x,x>x1])\n",
    "        x1 = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1_x2_y = pd.DataFrame(x1_x2_y,columns= [\"x1\",\"x2\",\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability x2 is greater than x1  is 0.7362341767571652\n",
      "probability x1 is greater than x2  is 0.2637658232428349\n"
     ]
    }
   ],
   "source": [
    "p_x2_gt_x1  = x1_x2_y[x1_x2_y.y == True].size/x1_x2_y.size\n",
    "p_x1_gt_x2  = x1_x2_y[x1_x2_y.y == False].size/x1_x2_y.size\n",
    "print(\"probability x2 is greater than x1  is {0}\".format(p_x2_gt_x1))\n",
    "print(\"probability x1 is greater than x2  is {0}\".format(p_x1_gt_x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $<X_1, X_2, .... X_t> $ 이 유저가 소유한 게임들의 시퀀스 일때 &nbsp; $$P(X_t > X_{t-1}) = 0.73$$ $$P(X_t < X_{t-1}) = 0.26$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: userOwns game list is ordered with release date (ascending). <- 기각 <br> H2: userOwns game list is ordered with release date (descending). <- 기각 <br> H3: userOwns game list is ordered with random. <- 보류 <br> H4: userOwns game list is ordered with purchased date. <- 보류\n",
    "\n",
    "#### 유저 게임 소유 데이터는 구매순으로 나열되어있다고 볼 수 있다.\n",
    "#### 데이터셋에 시간순서 $t$ 가 존재하므로 MDP (Markov Decision Process) 문제로 확대 가능 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12779 명의 유저가 10개이상의 게임을 구매함\n"
     ]
    }
   ],
   "source": [
    "usersHaveMore10Games = db.userOwns.find({\"game_count\":{\"$gte\":10}}) # 10개 이상의 게임을 구매한 고객\n",
    "print(\"{0} 명의 유저가 10개이상의 게임을 구매함\".format(usersHaveMore10Games.count())) # 12779 명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23252 : 모든 게임의 수(샘플 13642 명 유저 구매 기준)\n",
      "441   :  1000명 이상의 구매자가 있는 게임 수(샘플 13642 명 유저 구매 기준)\n",
      "22811 :  1000명 이하의 구매자가 있는 게임 수(샘플 13642 명 유저 구매 기준)\n"
     ]
    }
   ],
   "source": [
    "allGames = x1_x2_y.x1.append(x1_x2_y.x2).unique() \n",
    "print(\"{:<5d} : 모든 게임의 수(샘플 {:5d} 명 유저 구매 기준)\".format(len(allGames),userOwnsMoreThan1games.count()))\n",
    "\n",
    "topGames = x1_x2_y.x1.value_counts()[x1_x2_y.x1.value_counts() > 1000].index # 441개\n",
    "print(\"{:<5d} :  1000명 이상의 구매자가 있는 게임 수(샘플 {:5d} 명 유저 구매 기준)\".format(topGames.size,userOwnsMoreThan1games.count()))\n",
    "\n",
    "restGames = list(set(allGames) - set(topGames) )\n",
    "print(\"{:<5d} :  1000명 이하의 구매자가 있는 게임 수(샘플 {:5d} 명 유저 구매 기준)\".format(len(restGames),userOwnsMoreThan1games.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_rest_dic = {}   # dictionary to sort\n",
    "for i in topGames:\n",
    "    top_rest_dic.update({i:True})\n",
    "for i in restGames:\n",
    "    top_rest_dic.update({i:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self,s_arr):\n",
    "        self.state_arr = s_arr\n",
    "    def __str__(self):\n",
    "        return \"State([\"+\",\".join(map(lambda x:str(x),self.state_arr))+\"])\"\n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    def __eq__(self,other):\n",
    "        return self.state_arr == other.state_arr\n",
    "    def getAction(self):\n",
    "        return self.state_arr[-1]\n",
    "    def step(self,action):\n",
    "        return State(self.state_arr[1:] + [action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def queueProcess(x,q,g,g_rev):\n",
    "    nullState = State([-1])\n",
    "    if q.full():\n",
    "        state_1 = g[\"state_1\"]\n",
    "        state = State(list(q.queue))\n",
    "        if state_1 != nullState:\n",
    "            try:\n",
    "                g[state_1][state] += 1\n",
    "            except:\n",
    "                try:\n",
    "                    g[state_1].update({state:1})\n",
    "                except:\n",
    "                    g.update({state_1:{}})\n",
    "                    g[state_1].update({state:1})\n",
    "            try:\n",
    "                g[state]\n",
    "            except:\n",
    "                g.update({state:{}})\n",
    "                \n",
    "                \n",
    "            try:\n",
    "                g_rev[state][state_1] += 1\n",
    "            except:\n",
    "                try:\n",
    "                    g_rev[state].update({state_1:1})\n",
    "                except:\n",
    "                    g_rev.update({state:{}})\n",
    "                    g_rev[state].update({state_1:1})\n",
    "                \n",
    "        g[\"state_1\"] = state\n",
    "        q.get()\n",
    "        q.put(x)\n",
    "    else:\n",
    "        q.put(x)\n",
    "    return q,g,g_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form of DAG, $g_{t,t+n} $ is {$s_t$ : {$s_{t+1}$: $count \\ of \\ <s_t, s_{t+1}>$ , ....}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g1 = {}\n",
    "g1_2 = {}\n",
    "g1_rev = {}\n",
    "g1_2_rev = {}\n",
    "userOwnsMoreThan1games = db.userOwns.find({\"game_count\":{\"$gt\":1}})\n",
    "uomt1Size = userOwnsMoreThan1games.count()\n",
    "for _ in range(uomt1Size-1):\n",
    "    games = userOwnsMoreThan1games.next()['games']\n",
    "    q1 = Queue(maxsize=1)\n",
    "    q1_2 = Queue(maxsize=2)\n",
    "    g1[\"state_1\"]= State([-1])\n",
    "    g1_2[\"state_1\"]= State([-1])\n",
    "    for x in map(lambda x:x[\"appid\"],games):\n",
    "        if not top_rest_dic[x]:\n",
    "            continue\n",
    "        q1,g1,g1_rev =  queueProcess(x,q1,g1,g1_rev)\n",
    "        q1_2,g1_2,g1_2_rev =  queueProcess(x,q1_2,g1_2,g1_2_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('g1_2.txt', 'w') as outfile:\n",
    "    json.dump(str(g1_2), outfile)\n",
    "with open('g1_2_rev.txt', 'w') as outfile:\n",
    "    json.dump(str(g1_2_rev), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('g1_2.txt', 'r')\n",
    "\n",
    "context =  f.readlines()\n",
    "\n",
    "j = json.loads(context[0])\n",
    "\n",
    "g1_2 = eval(j)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('g1_2_rev.txt', 'r')\n",
    "\n",
    "context =  f.readlines()\n",
    "\n",
    "j = json.loads(context[0])\n",
    "\n",
    "g1_2_rev = eval(j)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the MDP\n",
    "the **state** is **sequence of games** recent t ~ t-n bought\n",
    "$$S_t = <g_t> \\ \\ or$$ \n",
    "\n",
    "$$S_t = <g_{t-1}, g_t>\\ \\ or$$\n",
    "\n",
    "$$S_t = <g_{t-2}, g_{t-1}, g_t>\\ \\ or$$\n",
    "\n",
    "$$S_t = <g_{t-3},g_{t-2}, g_{t-1}, g_t>$$\n",
    "\n",
    "\n",
    "the **action** of MDP correspond to a **recommendation of an game** (action 은 game 추천) <br>\n",
    "the **rewards** of MDP correspond to a $r(s_{t+1},a,s_t)= \\{w_1 \\times count( \\ \\cdot \\ ,s_{t+1})\\} \\  + \\ \\{w_2 \\times count(s_{t+1}, \\ \\cdot \\ )\\} \\ ,  \\ \\ where \\ \\ w_1 + w_2 = 1 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transition probability &nbsp; $tr_{MDP}(<x_1, x_2, x_3>, \\ x', \\ < x_2, x_3,x''>)$ <br>\n",
    "meaning that $P( s_{t+1} \\ | \\ s_t,a_t)$ &nbsp; $where$  &nbsp; $s_t = <x_1, x_2, x_3> $ , &nbsp; $s_{t+1} = <x_2, x_3,x''> $ , $a_t = x' $\n",
    "<br><br>\n",
    "assumptions of initial transition probability\n",
    "\n",
    "- A recommendation(action) increases the probability that a user will buy an game.\n",
    "    - This probability is proportional to the probability taht the user will buy this game in the absence of recommendation \n",
    "    - This assumption is made by most CF models dealing with e-commerce sites.\n",
    "    - we denote the proportionality constant  by  $\\alpha > 1$\n",
    "\n",
    "- The probability that a user will buy an game that was not recommened is lower than the probability that the agent will buy without recommendation.\n",
    "    - we denote the proportionality constant  by  $\\beta < 1$\n",
    "    \n",
    "    \n",
    "$$tr_{MDP}(<x_1, x_2, x_3>, \\ x', \\ < x_2, x_3,x'>) = \\alpha \\cdot tr_{MC}(<x_1, x_2, x_3>, \\ < x_2, x_3,x'>) $$\n",
    "\n",
    "$$tr_{MDP}(<x_1, x_2, x_3>, \\ x', \\ < x_2, x_3,x''>) = \\beta \\cdot tr_{MC}(<x_1, x_2, x_3>, \\ < x_2, x_3,x''>) \\ , \\ \\  x'' \\neq x'$$\n",
    "\n",
    "### Bellmann Equation\n",
    "$$v_\\pi(s) = \\sum\\limits_{a}\\pi(a|s)\\sum\\limits_{s'}tr_{MDP}[r+\\gamma v_\\pi(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rewards_g1 = {}\n",
    "rewards_g1_2 = {}\n",
    "#rewards_g1_3 = {}\n",
    "#g1_states_arr = list(g1.keys())\n",
    "g1_2_states_arr = list(g1_2.keys())\n",
    "#g1_3_states_arr = list(g1_3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRewardHash(g,g_rev):\n",
    "    r_g = {}\n",
    "    g_states_arr = list(g.keys())\n",
    "    for i in g_states_arr:\n",
    "        if type(i)  != State:\n",
    "            continue;\n",
    "        try:\n",
    "            popularScore = sum(g[i].values())\n",
    "        except:\n",
    "            popularScore = 0\n",
    "            \n",
    "        try:\n",
    "            afterEffectScore = sum(g_rev[i].values())\n",
    "        except:\n",
    "            afterEffectScore = 0        \n",
    "            \n",
    "        reward_i = 0.5*popularScore+0.5* afterEffectScore\n",
    "        r_g.update({i:reward_i})\n",
    "    return r_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rewards_g1 = getRewardHash(g1,g1_rev)\n",
    "rewards_g1_2 = getRewardHash(g1_2,g1_2_rev)\n",
    "#rewards_g1_3 = getRewardHash(g1_3,g1_3_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transProbMat(g):\n",
    "    g_states =  list(g.keys())\n",
    "    s_size = len(g_states)\n",
    "    state_index = {s:idx for idx,s in enumerate(g_states)}\n",
    "    transProbMat = np.zeros([s_size,s_size])  # transition probability \n",
    "    for s in g_states:\n",
    "        if type(s) != State:\n",
    "            continue\n",
    "        g_s_ss = g[s]\n",
    "        g_s_ss_sum = sum(g_s_ss.values())\n",
    "        idxI = state_index[s]\n",
    "        for ss in list(g_s_ss.keys()):\n",
    "            idxJ = state_index[ss]\n",
    "            transProbMat[idxI][idxJ] = g_s_ss[ss]/g_s_ss_sum\n",
    "    return transProbMat,state_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the MDP using Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self,g_DAG,reward_dict,transProbMat,state_index):\n",
    "        self.g_DAG = g_DAG\n",
    "        self.reward_dict = reward_dict\n",
    "        self.transProbMat = transProbMat\n",
    "        self.state_index = state_index\n",
    "    \n",
    "    def get_reward(self,  state, action):\n",
    "        next_state = state.step(action)\n",
    "        return self.reward_dict[next_state]\n",
    "    \n",
    "    def get_transition_prob(self, state, action):\n",
    "        stateIDX = state_index[state]\n",
    "        next_stateIDX = state_index[state.step(action)]\n",
    "        return self.transProbMat[stateIDX][next_stateIDX]\n",
    "    \n",
    "    def get_transition_prob_arr(self, state):\n",
    "        stateIDX = state_index[state]\n",
    "        return self.transProbMat[stateIDX]\n",
    "    \n",
    "    def get_possibleActions(self,state):\n",
    "        next_states = list(self.g_DAG[state].keys())\n",
    "        return [i.getAction() for i in next_states]\n",
    "    \n",
    "    def get_all_states(self):\n",
    "        return list(self.g_DAG.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    transi_Alpha = 1.5\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.value_hash = {state:0.0 for state in env.get_all_states()}\n",
    "        self.policy_hash = {state:{a:0.0 for a in env.get_possibleActions(state)} for state in env.get_all_states()}\n",
    "        self.fill_policy_hash()\n",
    "        self.discount_factor = 0.9\n",
    "    \n",
    "    def fill_policy_hash(self):\n",
    "        for state in self.policy_hash:\n",
    "            next_state_len = len(self.policy_hash[state])\n",
    "            if next_state_len == 0:\n",
    "                continue\n",
    "            for a in self.policy_hash[state]:\n",
    "                self.policy_hash[state][a] = 1/next_state_len\n",
    "    \n",
    "    def transition_prob(self,state,action,state_action_probList):\n",
    "        index = env.state_index[state.step(action)]\n",
    "        state_action_probList[index] = min(self.transi_Alpha * state_action_probList[index],1)\n",
    "        actionProb = copy(state_action_probList[index])\n",
    "        if state_action_probList[index] == 1 or (sum(state_action_probList)-state_action_probList[index]) < 0.01:\n",
    "            beta = 0\n",
    "        else:\n",
    "            beta = (1 - state_action_probList[index])/(sum(state_action_probList)-state_action_probList[index])\n",
    "        state_action_probList = state_action_probList*beta\n",
    "        state_action_probList[index] = actionProb\n",
    "        return state_action_probList\n",
    "        \n",
    "    def policy_evaluation(self):\n",
    "        next_value_hash = {state:0.0 for state in env.get_all_states()}\n",
    "        \n",
    "        # Bellman Expectation Equation for the every states\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            stateProbList = self.env.get_transition_prob_arr(state)\n",
    "            for action in self.env.get_possibleActions(state):\n",
    "                \n",
    "                state_action_probList = copy(stateProbList)\n",
    "                state_action_probList = self.transition_prob(state,action,state_action_probList)\n",
    "                policyProb = self.get_policy(state)[action]\n",
    "                reward = self.env.get_reward(state,action)\n",
    "                nextStates = list(env.g_DAG[state].keys())\n",
    "                for next_state in nextStates:\n",
    "                    next_value = self.get_value(next_state)\n",
    "                    value += (policyProb *(reward + state_action_probList[env.state_index[next_state]]*self.discount_factor * next_value))\n",
    "            next_value_hash[state] = round(value, 2)\n",
    "\n",
    "        self.value_hash = next_value_hash\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_hash\n",
    "        for state in self.env.get_all_states():\n",
    "            value = -99999\n",
    "            max_actions = []\n",
    "            result = {}  # initialize the policy\n",
    "\n",
    "            # for every actions, calculate\n",
    "            # [reward + (discount factor) * transition probability*(next state value function)]\n",
    "            stateProbList = self.env.get_transition_prob_arr(state)\n",
    "            for action in self.env.get_possibleActions(state):\n",
    "                next_policy[state][action] = 0.0\n",
    "                next_state = state.step(action)\n",
    "                reward = self.env.get_reward(state,action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor *next_value\n",
    "\n",
    "                # We normally can't pick multiple actions in greedy policy.\n",
    "                # but here we allow multiple actions with same max values\n",
    "                if temp == value:\n",
    "                    max_actions.append(action)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_actions.clear()\n",
    "                    max_actions.append(action)\n",
    "\n",
    "            # probability of action\n",
    "            if len(max_actions) != 0:\n",
    "                prob = 1 / len(max_actions)\n",
    "\n",
    "                for actions in max_actions:\n",
    "                    next_policy[state][actions] = prob\n",
    "\n",
    "\n",
    "        self.policy_table = next_policy\n",
    "\n",
    "    # get action according to the current policy\n",
    "    def get_action(self, state):\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # return the action in the index\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index\n",
    "\n",
    "    # get policy of specific state\n",
    "    def get_policy(self, state):\n",
    "        return self.policy_hash[state]\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_hash[state], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g1.pop('state_1', None)\n",
    "g1_2.pop('state_1', None)\n",
    "#g1_3.pop('state_1', None)\n",
    "transProbMat,state_index = get_transProbMat(g1_2)\n",
    "env = Env(g1_2,rewards_g1_2,transProbMat,state_index)\n",
    "policy_iteration = PolicyIteration(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 done\n",
      "iteration 1 done\n",
      "iteration 2 done\n",
      "iteration 3 done\n",
      "iteration 4 done\n",
      "iteration 5 done\n",
      "iteration 6 done\n",
      "iteration 7 done\n",
      "iteration 8 done\n",
      "iteration 9 done\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    policy_iteration.policy_evaluation()\n",
    "    print(\"iteration %d done\"%i)\n",
    "#policy_iteration.value_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('value_hash.txt', 'w') as outfile:\n",
    "    json.dump(str(policy_iteration.value_hash), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_iteration.policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#policy_iteration.policy_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_same = []\n",
    "same = 0\n",
    "notsame = 0\n",
    "for i in policy_iteration.policy_hash.keys():\n",
    "    for j in policy_iteration.policy_hash[i]:\n",
    "        if policy_iteration.policy_hash[i][j] > 0.0:\n",
    "            if g1_2[i][i.step(j)] == max(g1_2[i].values()):\n",
    "                state_same.append((i,1))\n",
    "                same += 1\n",
    "            else:\n",
    "                state_same.append((i,0))\n",
    "                notsame +=1\n",
    "            break\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27668, 10635)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same,notsame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
