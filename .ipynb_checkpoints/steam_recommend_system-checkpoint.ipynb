{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP based Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from queue import  Queue\n",
    "from copy import  copy\n",
    "import json\n",
    "from matplotlib import  pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections\n",
      "   - appList\n",
      "   - appDetail\n",
      "   - userOwns\n",
      "   - userAchieve\n",
      "   - userSummary\n",
      "   - userBan\n",
      "   - appReview\n"
     ]
    }
   ],
   "source": [
    "cl = pymongo.MongoClient()\n",
    "db = cl.steam\n",
    "print(\"Collections\")\n",
    "for i in db.collection_names():\n",
    "    print(\"   - \"+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: is userOwns-data ordered with user-purchaed time ?\n",
    "## Q: 유저게임소유데이터( userOwns-data) 는 유저 구매순으로 정렬되어 있는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userOwnsMoreThan1games = db.userOwns.find({\"game_count\":{\"$gte\":1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uomt1Size = userOwnsMoreThan1games.count()\n",
    "x1_x2_y = []\n",
    "for _ in range(uomt1Size):\n",
    "    games = userOwnsMoreThan1games.next()['games']\n",
    "    x1 = -1\n",
    "    for x in map(lambda x:x[\"appid\"],games):\n",
    "        if x1 == -1:\n",
    "            x1 = x\n",
    "            continue\n",
    "        x1_x2_y.append([x1,x,x>x1])\n",
    "        x1 = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1_x2_y = pd.DataFrame(x1_x2_y,columns= [\"x1\",\"x2\",\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability x2 is greater than x1  is 0.7362341767571652\n",
      "probability x1 is greater than x2  is 0.2637658232428349\n"
     ]
    }
   ],
   "source": [
    "p_x2_gt_x1  = x1_x2_y[x1_x2_y.y == True].size/x1_x2_y.size\n",
    "p_x1_gt_x2  = x1_x2_y[x1_x2_y.y == False].size/x1_x2_y.size\n",
    "print(\"probability x2 is greater than x1  is {0}\".format(p_x2_gt_x1))\n",
    "print(\"probability x1 is greater than x2  is {0}\".format(p_x1_gt_x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $<X_1, X_2, .... X_t> $ 이 유저가 소유한 게임들의 시퀀스 일때 &nbsp; $$P(X_t > X_{t-1}) = 0.73$$ $$P(X_t < X_{t-1}) = 0.26$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: userOwns game list is ordered with release date (ascending). <- 기각 <br> H2: userOwns game list is ordered with release date (descending). <- 기각 <br> H3: userOwns game list is ordered with random. <- 보류 <br> H4: userOwns game list is ordered with purchased date. <- 보류\n",
    "\n",
    "#### 유저 게임 소유 데이터는 구매순으로 나열되어있다고 볼 수 있다.\n",
    "#### 데이터셋에 시간순서 $t$ 가 존재하므로 MDP (Markov Decision Process) 문제로 확대 가능 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12779 명의 유저가 10개이상의 게임을 구매함\n"
     ]
    }
   ],
   "source": [
    "usersHaveMore10Games = db.userOwns.find({\"game_count\":{\"$gte\":10}}) # 10개 이상의 게임을 구매한 고객\n",
    "print(\"{0} 명의 유저가 10개이상의 게임을 구매함\".format(usersHaveMore10Games.count())) # 12779 명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23252 : 모든 게임의 수(샘플 13642 명 유저 구매 기준)\n",
      "441   :  1000명 이상의 구매자가 있는 게임 수(샘플 13642 명 유저 구매 기준)\n",
      "22811 :  1000명 이하의 구매자가 있는 게임 수(샘플 13642 명 유저 구매 기준)\n"
     ]
    }
   ],
   "source": [
    "allGames = x1_x2_y.x1.append(x1_x2_y.x2).unique() \n",
    "print(\"{:<5d} : 모든 게임의 수(샘플 {:5d} 명 유저 구매 기준)\".format(len(allGames),userOwnsMoreThan1games.count()))\n",
    "\n",
    "topGames = x1_x2_y.x1.value_counts()[x1_x2_y.x1.value_counts() > 1000].index # 441개\n",
    "print(\"{:<5d} :  1000명 이상의 구매자가 있는 게임 수(샘플 {:5d} 명 유저 구매 기준)\".format(topGames.size,userOwnsMoreThan1games.count()))\n",
    "\n",
    "restGames = list(set(allGames) - set(topGames) )\n",
    "print(\"{:<5d} :  1000명 이하의 구매자가 있는 게임 수(샘플 {:5d} 명 유저 구매 기준)\".format(len(restGames),userOwnsMoreThan1games.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_rest_dic = {}   # dictionary to sort\n",
    "for i in topGames:\n",
    "    top_rest_dic.update({i:True})\n",
    "for i in restGames:\n",
    "    top_rest_dic.update({i:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self,s_arr):\n",
    "        self.state_arr = s_arr\n",
    "    def __str__(self):\n",
    "        return \"State([\"+\",\".join(map(lambda x:str(x),self.state_arr))+\"])\"\n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    def __eq__(self,other):\n",
    "        return self.state_arr == other.state_arr\n",
    "    def getAction(self):\n",
    "        return self.state_arr[-1]\n",
    "    def step(self,action):\n",
    "        return State(self.state_arr[1:] + [action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def queueProcess(x,q,g,g_rev):\n",
    "    nullState = State([-1])\n",
    "    if q.full():\n",
    "        state_1 = g[\"state_1\"]\n",
    "        state = State(list(q.queue))\n",
    "        if state_1 != nullState:\n",
    "            try:\n",
    "                g[state_1][state] += 1\n",
    "            except:\n",
    "                try:\n",
    "                    g[state_1].update({state:1})\n",
    "                except:\n",
    "                    g.update({state_1:{}})\n",
    "                    g[state_1].update({state:1})\n",
    "            try:\n",
    "                g[state]\n",
    "            except:\n",
    "                g.update({state:{}})\n",
    "                \n",
    "                \n",
    "            try:\n",
    "                g_rev[state][state_1] += 1\n",
    "            except:\n",
    "                try:\n",
    "                    g_rev[state].update({state_1:1})\n",
    "                except:\n",
    "                    g_rev.update({state:{}})\n",
    "                    g_rev[state].update({state_1:1})\n",
    "                \n",
    "        g[\"state_1\"] = state\n",
    "        q.get()\n",
    "        q.put(x)\n",
    "    else:\n",
    "        q.put(x)\n",
    "    return q,g,g_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form of DAG, $g_{t,t+n} $ is {$s_t$ : {$s_{t+1}$: $count \\ of \\ <s_t, s_{t+1}>$ , ....}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g1 = {}\n",
    "g1_2 = {}\n",
    "g1_rev = {}\n",
    "g1_2_rev = {}\n",
    "userOwnsMoreThan1games = db.userOwns.find({\"game_count\":{\"$gt\":1}})\n",
    "uomt1Size = userOwnsMoreThan1games.count()\n",
    "for _ in range(uomt1Size-1):\n",
    "    games = userOwnsMoreThan1games.next()['games']\n",
    "    q1 = Queue(maxsize=1)\n",
    "    q1_2 = Queue(maxsize=2)\n",
    "    g1[\"state_1\"]= State([-1])\n",
    "    g1_2[\"state_1\"]= State([-1])\n",
    "    for x in map(lambda x:x[\"appid\"],games):\n",
    "        if not top_rest_dic[x]:\n",
    "            continue\n",
    "        q1,g1,g1_rev =  queueProcess(x,q1,g1,g1_rev)\n",
    "        q1_2,g1_2,g1_2_rev =  queueProcess(x,q1_2,g1_2,g1_2_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('g1_2.txt', 'w') as outfile:\n",
    "    json.dump(str(g1_2), outfile)\n",
    "with open('g1_2_rev.txt', 'w') as outfile:\n",
    "    json.dump(str(g1_2_rev), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('g1_2.txt', 'r')\n",
    "\n",
    "context =  f.readlines()\n",
    "\n",
    "j = json.loads(context[0])\n",
    "\n",
    "g1_2 = eval(j)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('g1_2_rev.txt', 'r')\n",
    "\n",
    "context =  f.readlines()\n",
    "\n",
    "j = json.loads(context[0])\n",
    "\n",
    "g1_2_rev = eval(j)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the MDP\n",
    "the **state** is **sequence of games** recent t ~ t-n bought\n",
    "$$S_t = <g_t> \\ \\ or$$ \n",
    "\n",
    "$$S_t = <g_{t-1}, g_t>\\ \\ or$$\n",
    "\n",
    "$$S_t = <g_{t-2}, g_{t-1}, g_t>\\ \\ or$$\n",
    "\n",
    "$$S_t = <g_{t-3},g_{t-2}, g_{t-1}, g_t>$$\n",
    "\n",
    "\n",
    "the **action** of MDP correspond to a **recommendation of an game** (action 은 game 추천) <br>\n",
    "the **rewards** of MDP correspond to a $r(s_{t+1},a,s_t)= \\{w_1 \\times count( \\ \\cdot \\ ,s_{t+1})\\} \\  + \\ \\{w_2 \\times count(s_{t+1}, \\ \\cdot \\ )\\} \\ ,  \\ \\ where \\ \\ w_1 + w_2 = 1 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transition probability &nbsp; $tr_{MDP}(<x_1, x_2, x_3>, \\ x', \\ < x_2, x_3,x''>)$ <br>\n",
    "meaning that $P( s_{t+1} \\ | \\ s_t,a_t)$ &nbsp; $where$  &nbsp; $s_t = <x_1, x_2, x_3> $ , &nbsp; $s_{t+1} = <x_2, x_3,x''> $ , $a_t = x' $\n",
    "<br><br>\n",
    "assumptions of initial transition probability\n",
    "\n",
    "- A recommendation(action) increases the probability that a user will buy an game.\n",
    "    - This probability is proportional to the probability taht the user will buy this game in the absence of recommendation \n",
    "    - This assumption is made by most CF models dealing with e-commerce sites.\n",
    "    - we denote the proportionality constant  by  $\\alpha > 1$\n",
    "\n",
    "- The probability that a user will buy an game that was not recommened is lower than the probability that the agent will buy without recommendation.\n",
    "    - we denote the proportionality constant  by  $\\beta < 1$\n",
    "    \n",
    "    \n",
    "$$tr_{MDP}(<x_1, x_2, x_3>, \\ x', \\ < x_2, x_3,x'>) = \\alpha \\cdot tr_{MC}(<x_1, x_2, x_3>, \\ < x_2, x_3,x'>) $$\n",
    "\n",
    "$$tr_{MDP}(<x_1, x_2, x_3>, \\ x', \\ < x_2, x_3,x''>) = \\beta \\cdot tr_{MC}(<x_1, x_2, x_3>, \\ < x_2, x_3,x''>) \\ , \\ \\  x'' \\neq x'$$\n",
    "\n",
    "### Bellmann Equation\n",
    "$$v_\\pi(s) = \\sum\\limits_{a}\\pi(a|s)\\sum\\limits_{s'}tr_{MDP}[r+\\gamma v_\\pi(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewards_g1 = {}\n",
    "rewards_g1_2 = {}\n",
    "#rewards_g1_3 = {}\n",
    "#g1_states_arr = list(g1.keys())\n",
    "g1_2_states_arr = list(g1_2.keys())\n",
    "#g1_3_states_arr = list(g1_3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRewardHash(g,g_rev):\n",
    "    r_g = {}\n",
    "    g_states_arr = list(g.keys())\n",
    "    for i in g_states_arr:\n",
    "        if type(i)  != State:\n",
    "            continue;\n",
    "        try:\n",
    "            popularScore = sum(g[i].values())\n",
    "        except:\n",
    "            popularScore = 0\n",
    "            \n",
    "        try:\n",
    "            afterEffectScore = sum(g_rev[i].values())\n",
    "        except:\n",
    "            afterEffectScore = 0        \n",
    "            \n",
    "        reward_i = 0.5*popularScore+0.5* afterEffectScore\n",
    "        r_g.update({i:reward_i})\n",
    "    return r_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rewards_g1 = getRewardHash(g1,g1_rev)\n",
    "rewards_g1_2 = getRewardHash(g1_2,g1_2_rev)\n",
    "#rewards_g1_3 = getRewardHash(g1_3,g1_3_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transProbMat(g):\n",
    "    g_states =  list(g.keys())\n",
    "    s_size = len(g_states)\n",
    "    state_index = {s:idx for idx,s in enumerate(g_states)}\n",
    "    transProbMat = np.zeros([s_size,s_size])  # transition probability \n",
    "    for s in g_states:\n",
    "        if type(s) != State:\n",
    "            continue\n",
    "        g_s_ss = g[s]\n",
    "        g_s_ss_sum = sum(g_s_ss.values())\n",
    "        idxI = state_index[s]\n",
    "        for ss in list(g_s_ss.keys()):\n",
    "            idxJ = state_index[ss]\n",
    "            transProbMat[idxI][idxJ] = g_s_ss[ss]/g_s_ss_sum\n",
    "    return transProbMat,state_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the MDP using Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self,g_DAG,reward_dict,transProbMat,state_index):\n",
    "        self.g_DAG = g_DAG\n",
    "        self.reward_dict = reward_dict\n",
    "        self.transProbMat = transProbMat\n",
    "        self.state_index = state_index\n",
    "    \n",
    "    def get_reward(self,  state, action):\n",
    "        next_state = state.step(action)\n",
    "        return self.reward_dict[next_state]\n",
    "    \n",
    "    def get_transition_prob(self, state, action):\n",
    "        stateIDX = state_index[state]\n",
    "        next_stateIDX = state_index[state.step(action)]\n",
    "        return self.transProbMat[stateIDX][next_stateIDX]\n",
    "    \n",
    "    def get_transition_prob_arr(self, state):\n",
    "        stateIDX = state_index[state]\n",
    "        return self.transProbMat[stateIDX]\n",
    "    \n",
    "    def get_possibleActions(self,state):\n",
    "        next_states = list(self.g_DAG[state].keys())\n",
    "        return [i.getAction() for i in next_states]\n",
    "    \n",
    "    def get_all_states(self):\n",
    "        return list(self.g_DAG.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    transi_Alpha = 1.5\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.value_hash = {state:0.0 for state in env.get_all_states()}\n",
    "        self.policy_hash = {state:{a:0.0 for a in env.get_possibleActions(state)} for state in env.get_all_states()}\n",
    "        self.fill_policy_hash()\n",
    "        self.discount_factor = 0.9\n",
    "    \n",
    "    def fill_policy_hash(self):\n",
    "        for state in self.policy_hash:\n",
    "            next_state_len = len(self.policy_hash[state])\n",
    "            if next_state_len == 0:\n",
    "                continue\n",
    "            for a in self.policy_hash[state]:\n",
    "                self.policy_hash[state][a] = 1/next_state_len\n",
    "    \n",
    "    def transition_prob(self,state,action,state_action_probList):\n",
    "        index = env.state_index[state.step(action)]\n",
    "        state_action_probList[index] = min(self.transi_Alpha * state_action_probList[index],1)\n",
    "        actionProb = copy(state_action_probList[index])\n",
    "        if state_action_probList[index] == 1 or (sum(state_action_probList)-state_action_probList[index]) < 0.01:\n",
    "            beta = 0\n",
    "        else:\n",
    "            beta = (1 - state_action_probList[index])/(sum(state_action_probList)-state_action_probList[index])\n",
    "        state_action_probList = state_action_probList*beta\n",
    "        state_action_probList[index] = actionProb\n",
    "        return state_action_probList\n",
    "        \n",
    "    def policy_evaluation(self):\n",
    "        next_value_hash = {state:0.0 for state in env.get_all_states()}\n",
    "        \n",
    "        # Bellman Expectation Equation for the every states\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            stateProbList = self.env.get_transition_prob_arr(state)\n",
    "            for action in self.env.get_possibleActions(state):\n",
    "                \n",
    "                state_action_probList = copy(stateProbList)\n",
    "                state_action_probList = self.transition_prob(state,action,state_action_probList)\n",
    "                policyProb = self.get_policy(state)[action]\n",
    "                reward = self.env.get_reward(state,action)\n",
    "                nextStates = list(env.g_DAG[state].keys())\n",
    "                for next_state in nextStates:\n",
    "                    next_value = self.get_value(next_state)\n",
    "                    value += (policyProb *(reward + state_action_probList[env.state_index[next_state]]*self.discount_factor * next_value))\n",
    "            next_value_hash[state] = round(value, 2)\n",
    "\n",
    "        self.value_hash = next_value_hash\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_hash\n",
    "        for state in self.env.get_all_states():\n",
    "            value = -99999\n",
    "            max_actions = []\n",
    "            result = {}  # initialize the policy\n",
    "\n",
    "            # for every actions, calculate\n",
    "            # [reward + (discount factor) * transition probability*(next state value function)]\n",
    "            stateProbList = self.env.get_transition_prob_arr(state)\n",
    "            for action in self.env.get_possibleActions(state):\n",
    "                next_policy[state][action] = 0.0\n",
    "                next_state = state.step(action)\n",
    "                reward = self.env.get_reward(state,action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor *next_value\n",
    "\n",
    "                # We normally can't pick multiple actions in greedy policy.\n",
    "                # but here we allow multiple actions with same max values\n",
    "                if temp == value:\n",
    "                    max_actions.append(action)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_actions.clear()\n",
    "                    max_actions.append(action)\n",
    "\n",
    "            # probability of action\n",
    "            if len(max_actions) != 0:\n",
    "                prob = 1 / len(max_actions)\n",
    "\n",
    "                for actions in max_actions:\n",
    "                    next_policy[state][actions] = prob\n",
    "\n",
    "\n",
    "        self.policy_table = next_policy\n",
    "\n",
    "    # get action according to the current policy\n",
    "    def get_action(self, state):\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # return the action in the index\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index\n",
    "\n",
    "    # get policy of specific state\n",
    "    def get_policy(self, state):\n",
    "        return self.policy_hash[state]\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_hash[state], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g1.pop('state_1', None)\n",
    "g1_2.pop('state_1', None)\n",
    "#g1_3.pop('state_1', None)\n",
    "transProbMat,state_index = get_transProbMat(g1_2)\n",
    "env = Env(g1_2,rewards_g1_2,transProbMat,state_index)\n",
    "policy_iteration = PolicyIteration(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 done\n",
      "iteration 1 done\n",
      "iteration 2 done\n",
      "iteration 3 done\n",
      "iteration 4 done\n",
      "iteration 5 done\n",
      "iteration 6 done\n",
      "iteration 7 done\n",
      "iteration 8 done\n",
      "iteration 9 done\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    policy_iteration.policy_evaluation()\n",
    "    print(\"iteration %d done\"%i)\n",
    "#policy_iteration.value_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('value_hash.txt', 'w') as outfile:\n",
    "    json.dump(str(policy_iteration.value_hash), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_iteration.policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for listSize in range(1,11):\n",
    "    userOwnsMoreThan1games = db.userOwns.find({\"game_count\":{\"$gt\":1}})\n",
    "    userOwnsMoreThan1games\n",
    "    uomt1Size = userOwnsMoreThan1games.count()\n",
    "    record_sucess_fail = []\n",
    "    for _ in range(uomt1Size-1):\n",
    "        games = userOwnsMoreThan1games.next()['games']\n",
    "        appid_1 = -1\n",
    "        gameIds = list(map(lambda x:x[\"appid\"],games))\n",
    "        maxLen = len(gameIds)\n",
    "        for idx,x in enumerate(gameIds):\n",
    "            if not top_rest_dic[x]:\n",
    "                continue\n",
    "            appid = x\n",
    "            if appid_1 == -1:\n",
    "                appid_1 = appid\n",
    "                continue\n",
    "            state = State([appid_1,appid])\n",
    "            appid_1 = appid\n",
    "            try:\n",
    "                actions = env.get_possibleActions(state)\n",
    "            except:\n",
    "                continue\n",
    "            nextState_values = []\n",
    "            for action in actions:\n",
    "                next_state = state.step(action)\n",
    "                next_state_val = policy_iteration.value_hash[next_state]\n",
    "                nextState_values.append((next_state,next_state_val))\n",
    "            nextState_values = sorted(nextState_values,key=lambda x:x[1],reverse=True)\n",
    "            if maxLen == idx +1 :\n",
    "                break\n",
    "            realNext = state.step(gameIds[idx + 1])\n",
    "            if realNext in [i for i,_ in nextState_values[:min(len(nextState_values),listSize)]]:\n",
    "                record_sucess_fail.append(1)\n",
    "            else:\n",
    "                record_sucess_fail.append(0)\n",
    "    scores.append(sum(record_sucess_fail)/len(record_sucess_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(scores)*100)\n",
    "plt.title(\"Recommend Score\")\n",
    "plt.ylabel('RC score')\n",
    "plt.xlabel(\"recommend list size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
